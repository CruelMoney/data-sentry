{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on unseen text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to determine the performance of different embedding and algorithm on unseen text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  load the unseen text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_data = pd.read_csv(\"../clean_data/Cleaned_test_text_with_pii_\\\n",
    "2018_12_31_05_35_46_815414.csv\")\n",
    "train_data = pd.read_csv(\"../clean_data/Cleaned_train_text_with_pii_2018_12_29_07_26_56_266227.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "      <th>PII</th>\n",
       "      <th>Cleaned_text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wife marriage cup 7416 Smith Forks before pict...</td>\n",
       "      <td>Address</td>\n",
       "      <td>7416 Smith Forks</td>\n",
       "      <td>wife marriage cup 7416 smith forks before pict...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However send which. Suite 244 Nice market acce...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Suite 244</td>\n",
       "      <td>however send which suite 244 nice market accep...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0497 Kemp Lane Amount tough and fire until. Is...</td>\n",
       "      <td>Address</td>\n",
       "      <td>0497 Kemp Lane</td>\n",
       "      <td>0497 kemp lane amount tough and fire until iss...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model north receive nature effort 58162 France...</td>\n",
       "      <td>Address</td>\n",
       "      <td>58162 Frances Shoals Conniemouth, OH 71686</td>\n",
       "      <td>model north receive nature effort 58162 france...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Child already drive could. Begin such down cel...</td>\n",
       "      <td>Address</td>\n",
       "      <td>538 Gina Circles</td>\n",
       "      <td>child already drive could begin such down cell...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text   Labels  \\\n",
       "0  Wife marriage cup 7416 Smith Forks before pict...  Address   \n",
       "1  However send which. Suite 244 Nice market acce...  Address   \n",
       "2  0497 Kemp Lane Amount tough and fire until. Is...  Address   \n",
       "3  Model north receive nature effort 58162 France...  Address   \n",
       "4  Child already drive could. Begin such down cel...  Address   \n",
       "\n",
       "                                          PII  \\\n",
       "0                            7416 Smith Forks   \n",
       "1                                   Suite 244   \n",
       "2                              0497 Kemp Lane   \n",
       "3  58162 Frances Shoals Conniemouth, OH 71686   \n",
       "4                            538 Gina Circles   \n",
       "\n",
       "                                        Cleaned_text  Target  \n",
       "0  wife marriage cup 7416 smith forks before pict...       1  \n",
       "1  however send which suite 244 nice market accep...       1  \n",
       "2  0497 kemp lane amount tough and fire until iss...       1  \n",
       "3  model north receive nature effort 58162 france...       1  \n",
       "4  child already drive could begin such down cell...       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None                10000\n",
       "Phone_number        10000\n",
       "CreditCardNumber    10000\n",
       "Name                10000\n",
       "Address             10000\n",
       "SSN                 10000\n",
       "Email               10000\n",
       "Plates              10000\n",
       "Name: Labels, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accomplish entire the training process with a simple Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 denote no pii\n",
    "# 1 denote pii exists\n",
    "def binary_pii(label):\n",
    "    pii_label = 0\n",
    "    if label != \"None\":\n",
    "        pii_label = 1\n",
    "    return pii_label \n",
    "\n",
    "test_data[\"Target\"] = test_data['Labels'].apply(binary_pii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    # replace  . and a space with only a space, then amke all words lower case.\n",
    "    text = text.replace(\". \",\" \").replace(\",\",\"\").lower()\n",
    "    # get rid of the . at the end of each line. \n",
    "    cleaned_text = re.sub(\"\\.$\",\"\",text)\n",
    "    \n",
    "    return cleaned_text\n",
    " \n",
    "\n",
    "\n",
    "class text_clean:\n",
    "    \"\"\"\n",
    "    A class to help with cleaning text data. \n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X,pd.Series), \"The input data should be pandas Series.\"\n",
    "        X = X.apply(clean_text)\n",
    "        \n",
    "        \n",
    "        return X\n",
    "    # This will return the entire dataframe with changed columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "def _find_part_pii(text, model, sep = \" \"):\n",
    "    tokenized_text = text.split(sep)\n",
    "    \n",
    "    part_pii = model.wv.doesnt_match(tokenized_text)\n",
    "    \n",
    "    return part_pii    \n",
    "\n",
    "\n",
    "\n",
    "def _get_word2vec_matrix(pii_list, model):\n",
    "    # set the matrix dimensions\n",
    "    column_num = model.trainables.layer1_size\n",
    "    row_num = len(pii_list)\n",
    "    # initialized the matrix\n",
    "    pii2vec_mat = np.zeros((row_num, column_num))\n",
    "    # iterate through the pii_list and assign the vectors to matrix.\n",
    "    for index, ith_pii in enumerate(tqdm(pii_list)):\n",
    "        pii2vec_mat[index,:] = model.wv[ith_pii]\n",
    "    \n",
    "    return pii2vec_mat\n",
    "\n",
    "\n",
    "\n",
    "class word2vec_embed:\n",
    "    \"\"\"\n",
    "    A class to convert words/docs to vectors by applying word2vec \n",
    "    algorithm for training a classifier. \n",
    "    Also used for predicting new unseen text by assigning the model variable. \n",
    "    \"\"\"\n",
    "    def __init__(self, size = 100, window=5,min_count = 1, workers = 1, \\\n",
    "                 epochs = 5, model = None):\n",
    "        self.size = size\n",
    "        self.window = window\n",
    "        self.min_count = min_count \n",
    "        self.epochs =  epochs \n",
    "        # this is set to 1 to voids problem with multi-core training. \n",
    "        self.workers = workers \n",
    "        self.model = model\n",
    "        if model is not None:\n",
    "            self.size = model.trainables.layer1_size\n",
    "            self.window = model.window\n",
    "            self.min_count = model.min_count\n",
    "            \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        assert isinstance(X,pd.Series), \"The input data should be pandas \\\n",
    "        Series for word2vec.\"\n",
    "        \n",
    "        # tokenized the sentences \n",
    "        tokenized_sentences = [sentence.split(\" \") for sentence in X]\n",
    "        # build vocab and train the word2vec model. \n",
    "        model = Word2Vec(size = self.size, window = self.window\\\n",
    "                   ,min_count = self.min_count, workers = self.workers )\n",
    "        \n",
    "\n",
    "        model.build_vocab(tokenized_sentences)\n",
    "            \n",
    "        \n",
    "        model.train(tokenized_sentences, total_examples = len(tokenized_sentences),\\\n",
    "                    epochs = self.epochs)\n",
    "        \n",
    "        # assign the trained model to self.model and return self.\n",
    "        self.model = model\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # update the embedding with new sentences. \n",
    "        model_updated = self.model\n",
    "        tokenized_new_sentences = [new_sentence.split(\" \") for new_sentence in X]\n",
    "        model_updated.build_vocab(tokenized_new_sentences, update = True)\n",
    "        model_updated.train(tokenized_new_sentences,\\\n",
    "                                  total_examples = len(tokenized_new_sentences)\\\n",
    "                                  , epochs = self.epochs)\n",
    "        # extract the PII \n",
    "        extracted_pii_list = [_find_part_pii(text, model = model_updated)\\\n",
    "                    for text in tqdm(X) ]\n",
    "        \n",
    "        # convert the extract pii text into vectors.\n",
    "        pii_matrix = _get_word2vec_matrix(pii_list = extracted_pii_list,\\\n",
    "                                          model = model_updated)\n",
    "        return pii_matrix \n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class doc2vec_embed:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fasttext_embed:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_clf = LogisticRegression(solver = \"lbfgs\", max_iter = 10000, class_weight={0:0.8,1:0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([('text_cleaning', text_clean()),\n",
    "                 (\"word2vec\", word2vec_embed()),\n",
    "                 (\"logit\",logit_clf)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800000/800000 [01:17<00:00, 10339.59it/s]\n",
      "100%|██████████| 800000/800000 [00:02<00:00, 279078.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 17s, sys: 11.3 s, total: 6min 28s\n",
      "Wall time: 6min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('text_cleaning', <__main__.text_clean object at 0x7f30e6591c88>), ('word2vec', <__main__.word2vec_embed object at 0x7f30e6591fd0>), ('logit', LogisticRegression(C=1.0, class_weight={0: 0.8, 1: 0.1}, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=10000,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(train_data[\"Text\"],train_data[\"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [00:09<00:00, 8826.04it/s] \n",
      "100%|██████████| 80000/80000 [00:00<00:00, 202371.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.8 s, sys: 433 ms, total: 25.2 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "binary_pred = model.predict(test_data[\"Text\"])\n",
    "binary_true = test_data[\"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 368 ms, sys: 141 ms, total: 509 ms\n",
      "Wall time: 319 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize= (8,8))\n",
    "sns.heatmap(confusion_matrix(y_true = binary_true, y_pred = binary_pred), annot = True,fmt=\"d\",cmap=\"YlGnBu\")\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"True values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.99      0.77     10000\n",
      "           1       1.00      0.92      0.96     70000\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     80000\n",
      "   macro avg       0.81      0.95      0.86     80000\n",
      "weighted avg       0.95      0.92      0.93     80000\n",
      "\n",
      "CPU times: user 72.9 ms, sys: 88.3 ms, total: 161 ms\n",
      "Wall time: 57.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true = binary_true, y_pred = binary_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance was quite good for a first relase. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
