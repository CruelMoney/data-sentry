{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to develop an end2end text classifier using different embedding like word2vec, fasttext, and doc2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pipe is to clean the text data (More cleaning for future data.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Data_science\\Python3.7\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning\n",
    "# ===========================================================================\n",
    "import re\n",
    "def clean_text(text):\n",
    "    # replace  . and a space with only a space, then amke all words lower case.\n",
    "    text = text.replace(\". \",\" \").replace(\",\",\"\").lower()\n",
    "    # get rid of the . at the end of each line. \n",
    "    cleaned_text = re.sub(\"\\.$\",\"\",text)\n",
    "    \n",
    "    return cleaned_text\n",
    " \n",
    "\n",
    "\n",
    "class text_clean:\n",
    "    \"\"\"\n",
    "    A class to help with cleaning text data. \n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X,pd.Series), \"The input data should be pandas Series.\"\n",
    "        X = X.apply(clean_text)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "# Word embedding training \n",
    "# ===========================================================================\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "def _find_part_pii(text, model, sep = \" \"):\n",
    "    tokenized_text = text.split(sep)\n",
    "    \n",
    "    part_pii = model.wv.doesnt_match(tokenized_text)\n",
    "    \n",
    "    return part_pii    \n",
    "\n",
    "\n",
    "\n",
    "def _extracted_pii2matrix(pii_list, model):\n",
    "    # set the matrix dimensions\n",
    "    column_num = model.trainables.layer1_size\n",
    "    row_num = len(pii_list)\n",
    "    # initialized the matrix\n",
    "    pii2vec_mat = np.zeros((row_num, column_num))\n",
    "    # iterate through the pii_list and assign the vectors to matrix.\n",
    "    for index, ith_pii in enumerate(tqdm(pii_list)):\n",
    "        pii2vec_mat[index,:] = model.wv[ith_pii]\n",
    "    \n",
    "    return pii2vec_mat\n",
    "\n",
    "\n",
    "\n",
    "class word_embedding:\n",
    "    \"\"\"\n",
    "    A class to convert words/docs to vectors by applying any model supported by gensim.  \n",
    "    \n",
    "    This class will allow continued training on the pre-trained model by assigning\n",
    "    the model to the pre_trained option in class initialization.  \n",
    "    \n",
    "    After training the model, it will dump the word2vec model to the path assigned to \n",
    "    dump_file option.  \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, algo_name = \"word2vec\", size = 100, min_count = 1, window = 5, workers =1,\\\n",
    "                 epochs = 5, pre_train = None, dump_file = False, continue_train_pre_train = True,\n",
    "                 re_train_new_sentences = True):\n",
    "        \n",
    "        \n",
    "        assert algo_name in [\"word2vec\", 'fasttext', 'doc2vec'], \\\n",
    "        \"please enter a model name in ['word2vec', 'fasttext', 'doc2vec']\"\n",
    "        \n",
    "        self.algo_name = algo_name\n",
    "        self.epochs = epochs \n",
    "        self.pre_train = pre_train\n",
    "        self.dump_file = dump_file \n",
    "        self.re_train_new_sentences = re_train_new_sentences\n",
    "        self.continue_train_pre_train = continue_train_pre_train\n",
    "        \n",
    "        # model options\n",
    "        self.size = size\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.workers = workers\n",
    "        \n",
    "        \n",
    "    def _algo_init(self):\n",
    "        if self.algo_name == \"word2vec\":\n",
    "            model = Word2Vec(size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "        elif self.algo_name == \"fasttext\":\n",
    "            model = FastText(size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "        elif self.algo_name == \"doc2vec\":\n",
    "            model = Doc2Vec(vector_size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "            \n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def _embedding_training(self, sentences, update = False):\n",
    "        \"\"\"\n",
    "        This helper functions will build the vocabulary, train the model and update the self.model\n",
    "        with the newly trained model.\n",
    "        \n",
    "        if update = True, it will update the vocabulary and the model can continue to train.\n",
    "        If update = False, the model will rebuild a new vocabulary from scratch using the input data.\n",
    "        \"\"\"\n",
    "        updated_model_with_vocab = self.model\n",
    "\n",
    "        updated_model_with_vocab.build_vocab(sentences, update = update)\n",
    "        \n",
    "        updated_model_with_vocab.train(sentences, total_examples = len(sentences), epochs = self.epochs)\n",
    "        \n",
    "        # update the model with the trained one. \n",
    "        self.model = updated_model_with_vocab\n",
    "        \n",
    "    def _pd_to_gensim_format(self, text):\n",
    "        \n",
    "        # special handling for doc2vec model. \n",
    "        if self.algo_name == \"doc2vec\":\n",
    "            documents = [TaggedDocument(sentence.split(\" \"), [index])\\\n",
    "                          for index, sentence in enumerate(text)] \n",
    "            print(\"Using index for the tags\")    \n",
    "        else:\n",
    "            documents = [sentence.split(\" \") for sentence in text]\n",
    "            \n",
    "            \n",
    "        return documents\n",
    "            \n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        The fit method will get use the pre_trained model if the model is assigned to the pre_train option.\n",
    "        \n",
    "        If the pre_train is None, then the model will be trained. \n",
    "        \n",
    "        \n",
    "        If the pre_train model is not None, then the default is to continue training on the new model. \n",
    "        Unless option continue_train_pre_train is specified as False. The False option will just assign \n",
    "        the pre_train model to self.model\n",
    "        \"\"\"\n",
    "        gensim_X = self._pd_to_gensim_format(text = X)\n",
    "        \n",
    "        if self.pre_train is not None:\n",
    "            \n",
    "            # update the pre_trained model with new training data\n",
    "            if self.continue_train_pre_train:\n",
    "                self.model = self.pre_train\n",
    "                self._embedding_training(sentences = gensim_X, update = True)\n",
    "                print(\"continue training with the pre_train model.\")\n",
    "                \n",
    "            # no training the pre_trained model. \n",
    "            elif not self.continue_train_pre_train:\n",
    "                self.model = self.pre_train\n",
    "                print(\"No training with pre_train model.\")\n",
    "                \n",
    "            \n",
    "            return self\n",
    "        \n",
    "        else:\n",
    "            # initialize the model, split the sentence into tokens and train it. \n",
    "            self._algo_init()\n",
    "            self._embedding_training(sentences = gensim_X)\n",
    "            print(\"Building new vocabulary and training the {} model\".format(self.algo_name))\n",
    "        \n",
    "        \n",
    "        # dump the model to disk\n",
    "        if isinstance(self.dump_file,str):\n",
    "            self.model.save(self.dump_file)\n",
    "            print(\"Writing the {} model to disk.\".format(self.algo_name))\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        If re_train_new_sentences is True, which is the default setting, \n",
    "        the model will be re-trained on the new sentences. \n",
    "        This will create word embedding for words not in the original vocabulary.\n",
    "        This will increase the model inference time since it invovles model training. \n",
    "        \n",
    "        For using word2vec to predict PII data, it is recommended to update the model with new sentences. \n",
    "        For fastttext, it is not necessary since it will infer from the character n-grams. The fasttext training\n",
    "        is much longer than word2vec. \n",
    "        \"\"\"\n",
    "        gensim_X = self._pd_to_gensim_format(text = X)\n",
    "        \n",
    "        # update the embedding with new sentences or train the model. \n",
    "        if self.re_train_new_sentences:\n",
    "            self._embedding_training(sentences = gensim_X, update = True)\n",
    "            print(\"transforming while training {} model with new data.\".format(self.algo_name))\n",
    "            \n",
    "            \n",
    "        # extract the PII \n",
    "        extracted_pii_list = [_find_part_pii(text = text, model = self.model)\\\n",
    "                    for text in tqdm(X) ]\n",
    "        \n",
    "        # convert the extracted pii text into vectors.\n",
    "        piivec_matrix = _extracted_pii2matrix(pii_list = extracted_pii_list,\\\n",
    "                                          model = self.model)\n",
    "        return piivec_matrix \n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "algo_test_data = pd.read_csv(\"../data/train_text_with_pii_2019_01_05_02_48_24_796403.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n",
      "Writing the word2vec model to disk.\n",
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 8268.32it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 160424.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding = word_embedding(algo_name = 'word2vec', dump_file = './devs/word2vec.bin')\n",
    "testing_embedding.fit(algo_test_data['Text']);\n",
    "test_pii_matrix = testing_embedding.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue training with the pre_train model.\n",
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 8022.81it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 89112.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained1 = Word2Vec.load(\"./word2vec/word2vec_cleaned_300_.bin\")\n",
    "pre_trained2 = Word2Vec.load(\"./devs/word2vec.bin\")\n",
    "testing_pre_trained = word_embedding(algo_name = \"word2vec\", pre_train = pre_trained2)\n",
    "testing_pre_trained.fit(algo_test_data[\"Text\"])\n",
    "test_pii_matrix = testing_pre_trained.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the fasttext model\n",
      "Writing the fasttext model to disk.\n",
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 6645.24it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 89124.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding = word_embedding(algo_name = 'fasttext', dump_file = \"./devs/fasttext.bin\")\n",
    "\n",
    "testing_embedding.fit(algo_test_data[\"Text\"]);\n",
    "test_pii_matrix = testing_embedding.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue training with the pre_train model.\n",
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 6742.05it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 100336.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained = FastText.load(\"./devs/fasttext.bin\")\n",
    "testing_pre_trained = word_embedding(algo_name = \"fasttext\", pre_train = pre_trained)\n",
    "testing_pre_trained.fit(algo_test_data[\"Text\"])\n",
    "test_pii_matrix = testing_pre_trained.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n",
      "Building new vocabulary and training the doc2vec model\n",
      "Writing the doc2vec model to disk.\n",
      "Using index for the tags\n",
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 11038.22it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 200384.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding_doc2vec = word_embedding(algo_name = 'doc2vec', dump_file = \"./devs/doc2vec.bin\")\n",
    "testing_embedding_doc2vec.fit(X = algo_test_data['Text'])\n",
    "\n",
    "test__doc2vec_pii_matrix = testing_embedding_doc2vec.transform(algo_test_data['Text'])\n",
    "test__doc2vec_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n",
      "continue training with the pre_train model.\n",
      "Using index for the tags\n",
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 9218.96it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 201358.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained = FastText.load(\"./devs/doc2vec.bin\")\n",
    "testing_pre_trained = word_embedding(algo_name = \"doc2vec\", pre_train = pre_trained)\n",
    "testing_pre_trained.fit(algo_test_data[\"Text\"])\n",
    "test_pii_matrix = testing_pre_trained.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
