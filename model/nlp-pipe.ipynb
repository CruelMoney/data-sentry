{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to develop an end2end text classifier using different embedding like word2vec, fasttext, and doc2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pipe is to clean the text data (More cleaning for future data.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Data cleaning\n",
    "# ===========================================================================\n",
    "import re\n",
    "def clean_text(text):\n",
    "    # replace  . and a space with only a space, then amke all words lower case.\n",
    "    text = text.replace(\". \",\" \").replace(\",\",\"\").lower()\n",
    "    # get rid of the . at the end of each line. \n",
    "    cleaned_text = re.sub(\"\\.$\",\"\",text)\n",
    "    \n",
    "    return cleaned_text\n",
    " \n",
    "\n",
    "\n",
    "class text_clean:\n",
    "    \"\"\"\n",
    "    A class to help with cleaning text data. \n",
    "    \"\"\"\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X,pd.Series), \"The input data should be pandas Series.\"\n",
    "        X = X.apply(clean_text)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "# Word embedding training \n",
    "# ===========================================================================\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "def _find_part_pii(text, model, sep = \" \"):\n",
    "    tokenized_text = text.split(sep)\n",
    "    \n",
    "    part_pii = model.wv.doesnt_match(tokenized_text)\n",
    "    \n",
    "    return part_pii    \n",
    "\n",
    "\n",
    "\n",
    "def _extracted_pii2matrix(pii_list, model):\n",
    "    # set the matrix dimensions\n",
    "    column_num = model.trainables.layer1_size\n",
    "    row_num = len(pii_list)\n",
    "    # initialized the matrix\n",
    "    pii2vec_mat = np.zeros((row_num, column_num))\n",
    "    # iterate through the pii_list and assign the vectors to matrix.\n",
    "    for index, ith_pii in enumerate(tqdm(pii_list)):\n",
    "        pii2vec_mat[index,:] = model.wv[ith_pii]\n",
    "    \n",
    "    return pii2vec_mat\n",
    "\n",
    "\n",
    "\n",
    "class word_embedding(BaseEstimator):\n",
    "    \"\"\"\n",
    "    A class to convert words/docs to vectors by applying any model supported by gensim.  \n",
    "    \n",
    "    This class will allow continued training on the pre-trained model by assigning\n",
    "    the model to the pre_trained option in class initialization.  \n",
    "    \n",
    "    After training the model, it will dump the word2vec model to the path assigned to \n",
    "    dump_file option.  \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, algo_name = \"word2vec\", size = 100, min_count = 1, window = 5, workers =1,\\\n",
    "                 epochs = 5, pre_train = None, dump_file = False, continue_train_pre_train = True,\n",
    "                 re_train_new_sentences = True):\n",
    "        \n",
    "        \n",
    "        assert algo_name in [\"word2vec\", 'fasttext', 'doc2vec'], \\\n",
    "        \"please enter a model name in ['word2vec', 'fasttext', 'doc2vec']\"\n",
    "        \n",
    "        self.algo_name = algo_name\n",
    "        self.epochs = epochs \n",
    "        self.pre_train = pre_train\n",
    "        self.dump_file = dump_file \n",
    "        self.re_train_new_sentences = re_train_new_sentences\n",
    "        self.continue_train_pre_train = continue_train_pre_train\n",
    "        \n",
    "        # model options\n",
    "        self.size = size\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.workers = workers\n",
    "        \n",
    "        \n",
    "    def _algo_init(self):\n",
    "        if self.algo_name == \"word2vec\":\n",
    "            model = Word2Vec(size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "        elif self.algo_name == \"fasttext\":\n",
    "            model = FastText(size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "        elif self.algo_name == \"doc2vec\":\n",
    "            model = Doc2Vec(vector_size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "            \n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def _embedding_training(self, sentences, update = False):\n",
    "        \"\"\"\n",
    "        This helper functions will build the vocabulary, train the model and update the self.model\n",
    "        with the newly trained model.\n",
    "        \n",
    "        if update = True, it will update the vocabulary and the model can continue to train.\n",
    "        If update = False, the model will rebuild a new vocabulary from scratch using the input data.\n",
    "        \"\"\"\n",
    "        updated_model_with_vocab = self.model\n",
    "\n",
    "        updated_model_with_vocab.build_vocab(sentences, update = update)\n",
    "        \n",
    "        updated_model_with_vocab.train(sentences, total_examples = len(sentences), epochs = self.epochs)\n",
    "        \n",
    "        # update the model with the trained one. \n",
    "        self.model = updated_model_with_vocab\n",
    "        \n",
    "    def _pd_to_gensim_format(self, text):\n",
    "        \n",
    "        # special handling for doc2vec model. \n",
    "        if self.algo_name == \"doc2vec\":\n",
    "            documents = [TaggedDocument(sentence.split(\" \"), [index])\\\n",
    "                          for index, sentence in enumerate(text)] \n",
    "            print(\"Using index for the tags\")    \n",
    "        else:\n",
    "            documents = [sentence.split(\" \") for sentence in text]\n",
    "            \n",
    "            \n",
    "        return documents\n",
    "            \n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\"\n",
    "        The fit method will get use the pre_trained model if the model is assigned to the pre_train option.\n",
    "        \n",
    "        If the pre_train is None, then the model will be trained. \n",
    "        \n",
    "        \n",
    "        If the pre_train model is not None, then the default is to continue training on the new model. \n",
    "        Unless option continue_train_pre_train is specified as False. The False option will just assign \n",
    "        the pre_train model to self.model\n",
    "        \"\"\"\n",
    "        gensim_X = self._pd_to_gensim_format(text = X)\n",
    "        \n",
    "        if self.pre_train is not None:\n",
    "            \n",
    "            # update the pre_trained model with new training data\n",
    "            if self.continue_train_pre_train:\n",
    "                self.model = self.pre_train\n",
    "                self._embedding_training(sentences = gensim_X, update = True)\n",
    "                print(\"continue training with the pre_train model.\")\n",
    "                \n",
    "            # no training the pre_trained model. \n",
    "            elif not self.continue_train_pre_train:\n",
    "                self.model = self.pre_train\n",
    "                print(\"No training with pre_train model.\")\n",
    "                \n",
    "            \n",
    "            return self\n",
    "        \n",
    "        else:\n",
    "            # initialize the model, split the sentence into tokens and train it. \n",
    "            self._algo_init()\n",
    "            self._embedding_training(sentences = gensim_X)\n",
    "            print(\"Building new vocabulary and training the {} model\".format(self.algo_name))\n",
    "        \n",
    "        \n",
    "        # dump the model to disk\n",
    "        if isinstance(self.dump_file,str):\n",
    "            self.model.save(self.dump_file)\n",
    "            print(\"Writing the {} model to disk.\".format(self.algo_name))\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        If re_train_new_sentences is True, which is the default setting, \n",
    "        the model will be re-trained on the new sentences. \n",
    "        This will create word embedding for words not in the original vocabulary.\n",
    "        This will increase the model inference time since it invovles model training. \n",
    "        \n",
    "        For using word2vec to predict PII data, it is recommended to update the model with new sentences. \n",
    "        For fastttext, it is not necessary since it will infer from the character n-grams. The fasttext training\n",
    "        is much longer than word2vec. \n",
    "        \"\"\"\n",
    "        gensim_X = self._pd_to_gensim_format(text = X)\n",
    "        \n",
    "        # update the embedding with new sentences or train the model. \n",
    "        if self.re_train_new_sentences:\n",
    "            self._embedding_training(sentences = gensim_X, update = True)\n",
    "            print(\"transforming while training {} model with new data.\".format(self.algo_name))\n",
    "            \n",
    "            \n",
    "        # extract the PII \n",
    "        extracted_pii_list = [_find_part_pii(text = text, model = self.model)\\\n",
    "                    for text in tqdm(X) ]\n",
    "        \n",
    "        # convert the extracted pii text into vectors.\n",
    "        piivec_matrix = _extracted_pii2matrix(pii_list = extracted_pii_list,\\\n",
    "                                          model = self.model)\n",
    "        return piivec_matrix \n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "algo_test_data = pd.read_csv(\"../data/train_text_with_pii_2019_01_05_02_48_24_796403.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n",
      "Writing the word2vec model to disk.\n",
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 9731.68it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 185753.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding = word_embedding(algo_name = 'word2vec', dump_file = './devs/word2vec.bin')\n",
    "testing_embedding.fit(algo_test_data['Text']);\n",
    "test_pii_matrix = testing_embedding.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 9535.95it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 186185.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue training with the pre_train model.\n",
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained1 = Word2Vec.load(\"./word2vec/word2vec_cleaned_300_.bin\")\n",
    "pre_trained2 = Word2Vec.load(\"./devs/word2vec.bin\")\n",
    "testing_pre_trained = word_embedding(algo_name = \"word2vec\", pre_train = pre_trained2)\n",
    "testing_pre_trained.fit(algo_test_data[\"Text\"])\n",
    "test_pii_matrix = testing_pre_trained.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the fasttext model\n",
      "Writing the fasttext model to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 6654.80it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 131529.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding = word_embedding(algo_name = 'fasttext', dump_file = \"./devs/fasttext.bin\")\n",
    "\n",
    "testing_embedding.fit(algo_test_data[\"Text\"]);\n",
    "test_pii_matrix = testing_embedding.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue training with the pre_train model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 6978.38it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 154857.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained = FastText.load(\"./devs/fasttext.bin\")\n",
    "testing_pre_trained = word_embedding(algo_name = \"fasttext\", pre_train = pre_trained)\n",
    "testing_pre_trained.fit(algo_test_data[\"Text\"])\n",
    "test_pii_matrix = testing_pre_trained.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the doc2vec model\n",
      "Writing the doc2vec model to disk.\n",
      "Using index for the tags\n",
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 8453.53it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 210451.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding_doc2vec = word_embedding(algo_name = 'doc2vec', dump_file = \"./devs/doc2vec.bin\")\n",
    "testing_embedding_doc2vec.fit(X = algo_test_data['Text'])\n",
    "\n",
    "test__doc2vec_pii_matrix = testing_embedding_doc2vec.transform(algo_test_data['Text'])\n",
    "test__doc2vec_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n",
      "continue training with the pre_train model.\n",
      "Using index for the tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 9990.42it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 199538.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained = FastText.load(\"./devs/doc2vec.bin\")\n",
    "testing_pre_trained = word_embedding(algo_name = \"doc2vec\", pre_train = pre_trained)\n",
    "testing_pre_trained.fit(algo_test_data[\"Text\"])\n",
    "test_pii_matrix = testing_pre_trained.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add all the pipe above to a single pipeline for training an end-to-end model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_clf = LogisticRegression(solver = \"lbfgs\", max_iter = 10000, class_weight={0:0.9,1:0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_pipe = Pipeline([('text_cleaning', text_clean()),\n",
    "                 (\"fasttext\", word_embedding(algo_name = \"word2vec\", size= 100,\\\n",
    "                                             dump_file = \"./word2vec/word2vec_100_pipe_dump.bin\")),\n",
    "                 (\"logit\",logit_clf)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_test_data = pd.read_csv(\"../clean_data/Cleaned_test_text_with_pii_2018_12_31_05_35_46_815414.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Labels</th>\n",
       "      <th>PII</th>\n",
       "      <th>Cleaned_text</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wife marriage cup 7416 Smith Forks before pict...</td>\n",
       "      <td>Address</td>\n",
       "      <td>7416 Smith Forks</td>\n",
       "      <td>wife marriage cup 7416 smith forks before pict...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However send which. Suite 244 Nice market acce...</td>\n",
       "      <td>Address</td>\n",
       "      <td>Suite 244</td>\n",
       "      <td>however send which suite 244 nice market accep...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0497 Kemp Lane Amount tough and fire until. Is...</td>\n",
       "      <td>Address</td>\n",
       "      <td>0497 Kemp Lane</td>\n",
       "      <td>0497 kemp lane amount tough and fire until iss...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model north receive nature effort 58162 France...</td>\n",
       "      <td>Address</td>\n",
       "      <td>58162 Frances Shoals Conniemouth, OH 71686</td>\n",
       "      <td>model north receive nature effort 58162 france...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Child already drive could. Begin such down cel...</td>\n",
       "      <td>Address</td>\n",
       "      <td>538 Gina Circles</td>\n",
       "      <td>child already drive could begin such down cell...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text   Labels  \\\n",
       "0  Wife marriage cup 7416 Smith Forks before pict...  Address   \n",
       "1  However send which. Suite 244 Nice market acce...  Address   \n",
       "2  0497 Kemp Lane Amount tough and fire until. Is...  Address   \n",
       "3  Model north receive nature effort 58162 France...  Address   \n",
       "4  Child already drive could. Begin such down cel...  Address   \n",
       "\n",
       "                                          PII  \\\n",
       "0                            7416 Smith Forks   \n",
       "1                                   Suite 244   \n",
       "2                              0497 Kemp Lane   \n",
       "3  58162 Frances Shoals Conniemouth, OH 71686   \n",
       "4                            538 Gina Circles   \n",
       "\n",
       "                                        Cleaned_text  Target  \n",
       "0  wife marriage cup 7416 smith forks before pict...       1  \n",
       "1  however send which suite 244 nice market accep...       1  \n",
       "2  0497 kemp lane amount tough and fire until iss...       1  \n",
       "3  model north receive nature effort 58162 france...       1  \n",
       "4  child already drive could begin such down cell...       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n",
      "Writing the word2vec model to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 730/80000 [00:00<00:10, 7295.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [00:08<00:00, 9961.38it/s] \n",
      "100%|██████████| 80000/80000 [00:00<00:00, 273495.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.3 s, sys: 5.24 s, total: 42.5 s\n",
      "Wall time: 34.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('text_cleaning', <__main__.text_clean object at 0x7f7983150be0>), ('fasttext', word_embedding(algo_name='word2vec', continue_train_pre_train=True,\n",
       "        dump_file='./word2vec/word2vec_100_pipe_dump.bin', epochs=5,\n",
       "        min_count=1, pre_train=None, re_train_new_sentences=True, size=100,\n",
       "...enalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "word2vec_pipe.fit(pipe_test_data[\"Text\"],pipe_test_data['Target'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fasttext pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_clf = LogisticRegression(solver = \"lbfgs\", max_iter = 10000, class_weight={0:0.9,1:0.1})\n",
    "fasttext_pipe = Pipeline([('text_cleaning', text_clean()),\n",
    "                 (\"fasttext\", word_embedding(algo_name = \"fasttext\", size= 100,\\\n",
    "                                             dump_file = \"./fasttext/fasttext_100_pipe_dump.bin\")),\n",
    "                 (\"logit\",logit_clf)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the fasttext model\n",
      "Writing the fasttext model to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/80000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [00:08<00:00, 8935.57it/s] \n",
      "100%|██████████| 80000/80000 [00:00<00:00, 237005.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 8.38 s, total: 1min 55s\n",
      "Wall time: 1min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('text_cleaning', <__main__.text_clean object at 0x7f797853d748>), ('fasttext', word_embedding(algo_name='fasttext', continue_train_pre_train=True,\n",
       "        dump_file='./fasttext/fasttext_100_pipe_dump.bin', epochs=5,\n",
       "        min_count=1, pre_train=None, re_train_new_sentences=True, size=100,\n",
       "...enalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "fasttext_pipe.fit(pipe_test_data[\"Text\"],pipe_test_data['Target'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_clf = LogisticRegression(solver = \"lbfgs\", max_iter = 10000, class_weight={0:0.9,1:0.1})\n",
    "doc2vec_pipe = Pipeline([('text_cleaning', text_clean()),\n",
    "                 (\"doc2vec\", word_embedding(algo_name = \"doc2vec\", size= 100,\\\n",
    "                                             dump_file = \"./doc2vec/fasttext_100_pipe_dump.bin\")),\n",
    "                 (\"logit\",logit_clf)\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n",
      "Building new vocabulary and training the doc2vec model\n",
      "Writing the doc2vec model to disk.\n",
      "Using index for the tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 751/80000 [00:00<00:10, 7508.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80000/80000 [00:07<00:00, 10003.70it/s]\n",
      "100%|██████████| 80000/80000 [00:00<00:00, 267942.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.4 s, sys: 4.07 s, total: 54.5 s\n",
      "Wall time: 48.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('text_cleaning', <__main__.text_clean object at 0x7f79784e3b00>), ('doc2vec', word_embedding(algo_name='doc2vec', continue_train_pre_train=True,\n",
       "        dump_file='./doc2vec/fasttext_100_pipe_dump.bin', epochs=5,\n",
       "        min_count=1, pre_train=None, re_train_new_sentences=True, size=100,\n",
       "   ...enalty='l2', random_state=None,\n",
       "          solver='lbfgs', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "doc2vec_pipe.fit(pipe_test_data[\"Text\"],pipe_test_data['Target'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# put the above pipe into a CV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "logit_clf = LogisticRegression(solver = \"lbfgs\", max_iter = 10000, class_weight={0:0.9,1:0.1})\n",
    "\n",
    "diff_pipe = Pipeline([('text_cleaning', text_clean()),\n",
    "                 (\"word_embedding\", word_embedding()),\n",
    "                 (\"logit\",logit_clf)\n",
    "                ])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'word_embedding__algo_name':['word2vec', 'doc2vec','fasttext'],\n",
    "    'word_embedding__size':[100,200,300],   \n",
    "    'logit__C': uniform(0,10),\n",
    "    'logit__class_weight':[{0: 0.9, 1: 0.1}, {0: 0.8, 1: 0.2}, None]\n",
    "}\n",
    "\n",
    "diff_pipe_cv = RandomizedSearchCV(estimator = diff_pipe,param_distributions = param_grid, cv =2, \\\n",
    "                                  error_score = 0,n_iter = 10 , scoring = 'f1'\n",
    "                               ,return_train_score=True, n_jobs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 839/40000 [00:00<00:04, 8385.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9949.95it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 218822.03it/s]\n",
      "  1%|          | 421/40000 [00:00<00:09, 4205.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9968.71it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 248649.33it/s]\n",
      "  2%|▏         | 694/40000 [00:00<00:05, 6934.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:03<00:00, 10252.13it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 249969.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 677/40000 [00:00<00:05, 6767.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9912.83it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 252556.71it/s]\n",
      "  2%|▏         | 785/40000 [00:00<00:05, 7841.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:03<00:00, 10262.03it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 246974.37it/s]\n",
      "  2%|▏         | 679/40000 [00:00<00:05, 6786.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:03<00:00, 10074.06it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 252855.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 764/40000 [00:00<00:05, 7633.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:03<00:00, 10277.24it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 249771.71it/s]\n",
      "  2%|▏         | 684/40000 [00:00<00:05, 6838.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9442.99it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 220742.68it/s]\n",
      "  2%|▏         | 800/40000 [00:00<00:04, 7994.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:03<00:00, 10304.87it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 254407.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 679/40000 [00:00<00:05, 6789.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:03<00:00, 10063.43it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 250329.99it/s]\n",
      "  2%|▏         | 774/40000 [00:00<00:05, 7738.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:03<00:00, 10286.29it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 242423.95it/s]\n",
      "  2%|▏         | 703/40000 [00:00<00:05, 7025.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:03<00:00, 10050.57it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 249173.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1718/40000 [00:00<00:04, 7717.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9931.51it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 224371.12it/s]\n",
      "  2%|▏         | 958/40000 [00:00<00:17, 2221.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9510.54it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 225074.27it/s]\n",
      "  1%|          | 219/40000 [00:00<00:18, 2189.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9700.35it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 231027.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 591/40000 [00:00<00:06, 5909.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9337.02it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 225409.63it/s]\n",
      "  0%|          | 191/40000 [00:00<00:20, 1908.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9815.42it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 221135.76it/s]\n",
      "  0%|          | 162/40000 [00:00<00:24, 1618.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9464.18it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 233948.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the fasttext model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8592.40it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 218188.55it/s]\n",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8057.26it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 208521.67it/s]\n",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:05<00:00, 7973.24it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 213121.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the fasttext model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8391.33it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 216200.68it/s]\n",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8157.48it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 204484.24it/s]\n",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8034.00it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 215454.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the fasttext model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/40000 [00:00<1:39:38,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8996.76it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 234247.99it/s]\n",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8515.75it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 231643.76it/s]\n",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8843.60it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 190211.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the fasttext model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/40000 [00:00<1:42:19,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8729.48it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 233810.87it/s]\n",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8737.04it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 223993.98it/s]\n",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8548.26it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 239725.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1867/40000 [00:00<00:04, 8731.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:03<00:00, 10318.25it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 240752.05it/s]\n",
      "  2%|▏         | 700/40000 [00:00<00:05, 6997.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9063.32it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 243076.54it/s]\n",
      "  2%|▏         | 720/40000 [00:00<00:05, 7197.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9626.43it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 251029.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 714/40000 [00:00<00:05, 7135.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:03<00:00, 10056.51it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 247611.52it/s]\n",
      "  2%|▏         | 634/40000 [00:00<00:06, 6332.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9815.92it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 216190.09it/s]\n",
      "  2%|▏         | 612/40000 [00:00<00:06, 6117.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9068.69it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 250182.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 738/40000 [00:00<00:05, 7379.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9985.58it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 237160.61it/s]\n",
      "  2%|▏         | 982/40000 [00:00<00:19, 1987.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9373.12it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 232064.46it/s]\n",
      "  1%|          | 294/40000 [00:00<00:13, 2937.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9376.10it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 248268.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 499/40000 [00:00<00:07, 4989.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8620.40it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 194427.15it/s]\n",
      "  1%|          | 267/40000 [00:00<00:14, 2669.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9302.12it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 218466.70it/s]\n",
      "  0%|          | 125/40000 [00:00<00:32, 1233.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9056.63it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 219454.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n",
      "Building new vocabulary and training the doc2vec model\n",
      "Using index for the tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 807/40000 [00:00<00:04, 8066.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9399.03it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 207505.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 159/40000 [00:00<00:25, 1589.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9190.36it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 229655.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 276/40000 [00:00<00:14, 2759.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9549.74it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 204081.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n",
      "Building new vocabulary and training the doc2vec model\n",
      "Using index for the tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 497/40000 [00:00<00:07, 4962.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8865.69it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 240336.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 270/40000 [00:00<00:14, 2699.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9084.13it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 182038.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 134/40000 [00:00<00:29, 1338.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8673.61it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 187450.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 801/40000 [00:00<00:04, 8006.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9155.61it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 206110.08it/s]\n",
      "  2%|▏         | 683/40000 [00:00<00:05, 6827.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9654.29it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 260742.10it/s]\n",
      "  5%|▍         | 1915/40000 [00:00<00:04, 9103.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:03<00:00, 10137.26it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 268187.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the word2vec model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 607/40000 [00:00<00:06, 6069.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9280.90it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 259674.25it/s]\n",
      "  2%|▏         | 742/40000 [00:00<00:05, 7409.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9438.41it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 202346.26it/s]\n",
      "  2%|▏         | 742/40000 [00:00<00:05, 7416.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 9588.18it/s] \n",
      "100%|██████████| 40000/40000 [00:00<00:00, 269993.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the fasttext model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8289.93it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 217590.62it/s]\n",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:05<00:00, 7651.70it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 179075.78it/s]\n",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8025.18it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 224031.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building new vocabulary and training the fasttext model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/40000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:04<00:00, 8099.96it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 213128.18it/s]\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "diff_pipe_cv.fit(pipe_test_data[\"Text\"],pipe_test_data['Target'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
