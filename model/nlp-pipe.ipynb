{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to develop an end2end text classifier using different embedding like word2vec, fasttext, and doc2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pipe is to clean the text data (More cleaning for future data.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "# ===========================================================================\n",
    "import re\n",
    "def clean_text(text):\n",
    "    # replace  . and a space with only a space, then amke all words lower case.\n",
    "    text = text.replace(\". \",\" \").replace(\",\",\"\").lower()\n",
    "    # get rid of the . at the end of each line. \n",
    "    cleaned_text = re.sub(\"\\.$\",\"\",text)\n",
    "    \n",
    "    return cleaned_text\n",
    " \n",
    "\n",
    "\n",
    "class text_clean:\n",
    "    \"\"\"\n",
    "    A class to help with cleaning text data. \n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X,pd.Series), \"The input data should be pandas Series.\"\n",
    "        X = X.apply(clean_text)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "# Word embedding training \n",
    "# ===========================================================================\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "def _find_part_pii(text, model, sep = \" \"):\n",
    "    tokenized_text = text.split(sep)\n",
    "    \n",
    "    part_pii = model.wv.doesnt_match(tokenized_text)\n",
    "    \n",
    "    return part_pii    \n",
    "\n",
    "\n",
    "\n",
    "def _extracted_pii2matrix(pii_list, model):\n",
    "    # set the matrix dimensions\n",
    "    column_num = model.trainables.layer1_size\n",
    "    row_num = len(pii_list)\n",
    "    # initialized the matrix\n",
    "    pii2vec_mat = np.zeros((row_num, column_num))\n",
    "    # iterate through the pii_list and assign the vectors to matrix.\n",
    "    for index, ith_pii in enumerate(tqdm(pii_list)):\n",
    "        pii2vec_mat[index,:] = model.wv[ith_pii]\n",
    "    \n",
    "    return pii2vec_mat\n",
    "\n",
    "\n",
    "\n",
    "class word_embedding:\n",
    "    \"\"\"\n",
    "    A class to convert words/docs to vectors by applying any model supported by gensim.  \n",
    "    \n",
    "    This class will allow continued training on the pre-trained model by assigning\n",
    "    the model to the pre_trained option in class initialization.  \n",
    "    \n",
    "    After training the model, it will dump the word2vec model to the path assigned to \n",
    "    dump_file option.  \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, algo_name = \"word2vec\", size = 100, min_count = 1, window = 5, workers =1,\\\n",
    "                 epochs = 5, pre_train = None, dump_file = False,\\\n",
    "                 re_train_new_sentences = True):\n",
    "        \n",
    "        \n",
    "        assert algo_name in [\"word2vec\", 'fasttext', 'doc2vec'], \\\n",
    "        \"please enter a model name in ['word2vec', 'fasttext', 'doc2vec']\"\n",
    "        \n",
    "        self.algo_name = algo_name\n",
    "        self.epochs = epochs \n",
    "        self.pre_train = pre_train\n",
    "        self.dump_file = dump_file \n",
    "        self.re_train_new_sentences = re_train_new_sentences\n",
    "        \n",
    "        # model options\n",
    "        self.size = size\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.workers = workers\n",
    "        \n",
    "        \n",
    "    def _algo_init(self):\n",
    "        if self.algo_name == \"word2vec\":\n",
    "            model = Word2Vec(size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "        elif self.algo_name == \"fasttext\":\n",
    "            model = FastText(size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "        elif self.algo_name == \"doc2vec\":\n",
    "            model = Doc2Vec(vector_size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "            \n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def _embedding_training(self, sentences, update = False):\n",
    "        \"\"\"\n",
    "        if update = True, it will update the vocabulary and the model can continue to train.\n",
    "        If update = False, the model will rebuild a new vocabulary from scratch using the input data.\n",
    "        \"\"\"\n",
    "        updated_model_with_vocab = self.model\n",
    "\n",
    "        updated_model_with_vocab.build_vocab(sentences, update = update)\n",
    "        \n",
    "        updated_model_with_vocab.train(sentences, total_examples = len(sentences), epochs = self.epochs)\n",
    "        \n",
    "        # update the model with the trained one. \n",
    "        self.model = updated_model_with_vocab\n",
    "        \n",
    "    def _pd_to_gensim_format(self, text):\n",
    "        \n",
    "        # special handling for doc2vec model. \n",
    "        if self.algo_name == \"doc2vec\":\n",
    "            documents = [TaggedDocument(sentence.split(\" \"), [index])\\\n",
    "                          for index, sentence in enumerate(text)] \n",
    "            print(\"Using index for the tags\")    \n",
    "        else:\n",
    "            documents = [sentence.split(\" \") for sentence in text]\n",
    "            \n",
    "            \n",
    "        return documents\n",
    "            \n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        The fit method will get use the pre_trained model if the model is assigned to the pre_train option.\n",
    "        \n",
    "        If the pre_train is None, then the model will be trained. \n",
    "        \"\"\"\n",
    "        gensim_X = self._pd_to_gensim_format(text = X)\n",
    "        \n",
    "        if self.pre_train is not None:\n",
    "            self.model = self.pre_train\n",
    "            return self\n",
    "        else:\n",
    "            # initialize the model, split the sentence into tokens and train it. \n",
    "            self._algo_init()\n",
    "            self._embedding_training(sentences = gensim_X)\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        If re_train_new_sentences is True, which is the default setting, \n",
    "        the model will be re-trained on the new sentences. \n",
    "        This will create word embedding for words not in the original vocabulary.\n",
    "        This will increase the model inference time since it invovles model training. \n",
    "        \n",
    "        For using word2vec to predict PII data, it is recommended to update the model with new sentences. \n",
    "        For fastttext, it is not necessary since it will infer from the character n-grams. The fasttext training\n",
    "        is much longer than word2vec. \n",
    "        \"\"\"\n",
    "        gensim_X = self._pd_to_gensim_format(text = X)\n",
    "        # update the embedding with new sentences or train the model. \n",
    "        if self.re_train_new_sentences:\n",
    "            self._embedding_training(sentences = gensim_X, update = True)\n",
    "            print(\"transforming while training {} model with new data.\".format(self.algo_name))\n",
    "            \n",
    "            \n",
    "        # extract the PII \n",
    "        extracted_pii_list = [_find_part_pii(text = text, model = self.model)\\\n",
    "                    for text in tqdm(X) ]\n",
    "        \n",
    "        # convert the extracted pii text into vectors.\n",
    "        piivec_matrix = _extracted_pii2matrix(pii_list = extracted_pii_list,\\\n",
    "                                          model = self.model)\n",
    "        return piivec_matrix \n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "algo_test_data = pd.read_csv(\"../data/train_text_with_pii_2019_01_05_02_48_24_796403.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 9798.60it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 59669.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00552159, -0.00203225, -0.00489316, ..., -0.00241043,\n",
       "         0.00955387, -0.01075913],\n",
       "       [-0.00626223, -0.00024075, -0.00917153, ..., -0.00296824,\n",
       "         0.00430816, -0.00740499],\n",
       "       [-0.00933292,  0.00148898, -0.00867963, ..., -0.00629046,\n",
       "         0.00426485, -0.00276585],\n",
       "       ...,\n",
       "       [-0.02212575,  0.00074998, -0.02750545, ..., -0.02030576,\n",
       "         0.02142496, -0.02265347],\n",
       "       [-0.02438817,  0.00431983, -0.01989118, ..., -0.01755816,\n",
       "         0.01750636, -0.0174949 ],\n",
       "       [-0.01403672,  0.00744611, -0.02118406, ..., -0.00960288,\n",
       "         0.0180083 , -0.01796213]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding = word_embedding(algo_name = 'word2vec')\n",
    "testing_embedding.fit(algo_test_data['Text']);\n",
    "test_pii_matrix = testing_embedding.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 3529.54it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 41656.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.07526994, -0.18667389, -0.96105301, ...,  0.15214525,\n",
       "         0.02959202, -0.41313222],\n",
       "       [ 0.42987594, -0.05497638, -0.96129489, ...,  0.46197411,\n",
       "         0.22145101, -0.54831982],\n",
       "       [ 0.04325041, -0.0285845 , -0.00989333, ..., -0.05394333,\n",
       "         0.20068049,  0.08983152],\n",
       "       ...,\n",
       "       [-0.04774014, -0.05916891,  0.03238036, ..., -0.08683169,\n",
       "         0.08028586,  0.00404379],\n",
       "       [ 0.29659271, -0.58653063,  0.01698974, ...,  0.43566033,\n",
       "        -0.06582996,  1.55989099],\n",
       "       [ 0.15159334, -0.12143414, -0.10422158, ...,  0.03164278,\n",
       "         0.05929553,  0.09101988]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained = Word2Vec.load(\"./word2vec/word2vec_cleaned_300_.bin\")\n",
    "testing_pre_trained = word_embedding(algo_name = \"word2vec\", pre_train = pre_trained)\n",
    "testing_pre_trained.fit(algo_test_data[\"Text\"])\n",
    "testing_pre_trained.transform(algo_test_data[\"Text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 6808.63it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 89131.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00797034,  0.00529751, -0.02028651, ..., -0.05967641,\n",
       "        -0.00291255,  0.05439368],\n",
       "       [ 0.0006045 ,  0.00365394, -0.01218145, ..., -0.03478827,\n",
       "        -0.00018567,  0.030108  ],\n",
       "       [-0.00948382,  0.00533016, -0.03240121, ..., -0.08218741,\n",
       "        -0.00651771,  0.07036647],\n",
       "       ...,\n",
       "       [-0.02091836,  0.01299771, -0.05696366, ..., -0.15254602,\n",
       "        -0.0053024 ,  0.13926317],\n",
       "       [-0.02786561,  0.01796342, -0.06887486, ..., -0.17057717,\n",
       "        -0.00203732,  0.15210037],\n",
       "       [-0.02402436,  0.0121314 , -0.06329311, ..., -0.15296748,\n",
       "        -0.00263037,  0.13743119]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding = word_embedding(algo_name = 'fasttext')\n",
    "\n",
    "testing_embedding.fit(algo_test_data[\"Text\"]);\n",
    "test_pii_matrix = testing_embedding.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n",
      "Using index for the tags\n",
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 11117.11it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 93113.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00335109, -0.00195854, -0.00548842, ..., -0.00846553,\n",
       "        -0.00111197, -0.00612434],\n",
       "       [-0.0051255 , -0.00264326, -0.00670908, ..., -0.00396664,\n",
       "         0.00131375, -0.00429768],\n",
       "       [-0.0112185 , -0.00281456, -0.00480118, ..., -0.00295908,\n",
       "         0.00372413, -0.00662092],\n",
       "       ...,\n",
       "       [-0.01494441, -0.00622322, -0.01718825, ..., -0.02094651,\n",
       "         0.01046414, -0.01136488],\n",
       "       [-0.02091397, -0.00221723, -0.01378535, ..., -0.02076681,\n",
       "         0.0110327 , -0.00925473],\n",
       "       [-0.01569927,  0.00378848, -0.01961735, ..., -0.01383845,\n",
       "         0.00869041, -0.0135015 ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding_doc2vec = word_embedding(algo_name = 'doc2vec')\n",
    "testing_embedding_doc2vec.fit(X = algo_test_data['Text'])\n",
    "\n",
    "test__doc2vec_pii_matrix = testing_embedding_doc2vec.transform(algo_test_data['Text'])\n",
    "test__doc2vec_pii_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add transfer_learning option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add dum_file option"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
