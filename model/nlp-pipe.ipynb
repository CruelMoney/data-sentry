{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to develop an end2end text classifier using different embedding like word2vec, fasttext, and doc2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pipe is to clean the text data (More cleaning for future data.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "# ===========================================================================\n",
    "import re\n",
    "def clean_text(text):\n",
    "    # replace  . and a space with only a space, then amke all words lower case.\n",
    "    text = text.replace(\". \",\" \").replace(\",\",\"\").lower()\n",
    "    # get rid of the . at the end of each line. \n",
    "    cleaned_text = re.sub(\"\\.$\",\"\",text)\n",
    "    \n",
    "    return cleaned_text\n",
    " \n",
    "\n",
    "\n",
    "class text_clean:\n",
    "    \"\"\"\n",
    "    A class to help with cleaning text data. \n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X,pd.Series), \"The input data should be pandas Series.\"\n",
    "        X = X.apply(clean_text)\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "# Word embedding training \n",
    "# ===========================================================================\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "def _find_part_pii(text, model, sep = \" \"):\n",
    "    tokenized_text = text.split(sep)\n",
    "    \n",
    "    part_pii = model.wv.doesnt_match(tokenized_text)\n",
    "    \n",
    "    return part_pii    \n",
    "\n",
    "\n",
    "\n",
    "def _extracted_pii2matrix(pii_list, model):\n",
    "    # set the matrix dimensions\n",
    "    column_num = model.trainables.layer1_size\n",
    "    row_num = len(pii_list)\n",
    "    # initialized the matrix\n",
    "    pii2vec_mat = np.zeros((row_num, column_num))\n",
    "    # iterate through the pii_list and assign the vectors to matrix.\n",
    "    for index, ith_pii in enumerate(tqdm(pii_list)):\n",
    "        pii2vec_mat[index,:] = model.wv[ith_pii]\n",
    "    \n",
    "    return pii2vec_mat\n",
    "\n",
    "\n",
    "\n",
    "class word_embedding:\n",
    "    \"\"\"\n",
    "    A class to convert words/docs to vectors by applying any model supported by gensim.  \n",
    "    \n",
    "    This class will allow continued training on the pre-trained model by assigning\n",
    "    the model to the pre_trained option in class initialization.  \n",
    "    \n",
    "    After training the model, it will dump the word2vec model to the path assigned to \n",
    "    dump_file option.  \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, algo_name = \"word2vec\", size = 100, min_count = 1, window = 5, workers =1,\\\n",
    "                 epochs = 5, pre_train = None, dump_file = False,\\\n",
    "                 re_train_new_sentences = True):\n",
    "        \n",
    "        \n",
    "        assert algo_name in [\"word2vec\", 'fasttext', 'doc2vec'], \\\n",
    "        \"please enter a model name in ['word2vec', 'fasttext', 'doc2vec']\"\n",
    "        \n",
    "        self.algo_name = algo_name\n",
    "        self.epochs = epochs \n",
    "        self.pre_train = pre_train\n",
    "        self.dump_file = dump_file \n",
    "        self.re_train_new_sentences = re_train_new_sentences\n",
    "        \n",
    "        # model options\n",
    "        self.size = size\n",
    "        self.min_count = min_count\n",
    "        self.window = window\n",
    "        self.workers = workers\n",
    "        \n",
    "        \n",
    "    def _algo_init(self):\n",
    "        if self.algo_name == \"word2vec\":\n",
    "            model = Word2Vec(size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "        elif self.algo_name == \"fasttext\":\n",
    "            model = FastText(size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "        elif self.algo_name == \"doc2vec\":\n",
    "            model = Doc2Vec(vector_size = self.size, min_count = self.min_count,\n",
    "                            window = self.window, workers = self.workers)\n",
    "            \n",
    "        self.model = model\n",
    "        return self\n",
    "\n",
    "    def _embedding_training(self, sentences, update = False):\n",
    "        \"\"\"\n",
    "        if update = True, it will update the vocabulary and the model can continue to train.\n",
    "        If update = False, the model will rebuild a new vocabulary from scratch using the input data.\n",
    "        \"\"\"\n",
    "        updated_model_with_vocab = self.model\n",
    "\n",
    "        updated_model_with_vocab.build_vocab(sentences, update = update)\n",
    "        \n",
    "        updated_model_with_vocab.train(sentences, total_examples = len(sentences), epochs = self.epochs)\n",
    "        \n",
    "        # update the model with the trained one. \n",
    "        self.model = updated_model_with_vocab\n",
    "        \n",
    "    def _pd_to_gensim_format(self, text):\n",
    "        \n",
    "        # special handling for doc2vec model. \n",
    "        if self.algo_name == \"doc2vec\":\n",
    "            documents = [TaggedDocument(sentence.split(\" \"), [index])\\\n",
    "                          for index, sentence in enumerate(text)] \n",
    "            print(\"Using index for the tags\")    \n",
    "        else:\n",
    "            documents = [sentence.split(\" \") for sentence in text]\n",
    "            \n",
    "            \n",
    "        return documents\n",
    "            \n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        The fit method will get use the pre_trained model if the model is assigned to the pre_train option.\n",
    "        \n",
    "        If the pre_train is None, then the model will be trained. \n",
    "        \"\"\"\n",
    "        gensim_X = self._pd_to_gensim_format(text = X)\n",
    "        \n",
    "        if self.pre_train is not None:\n",
    "            self.model = self.pre_train\n",
    "            return self\n",
    "        else:\n",
    "            # initialize the model, split the sentence into tokens and train it. \n",
    "            self._algo_init()\n",
    "            self._embedding_training(sentences = gensim_X)\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        If re_train_new_sentences is True, which is the default setting, \n",
    "        the model will be re-trained on the new sentences. \n",
    "        This will create word embedding for words not in the original vocabulary.\n",
    "        This will increase the model inference time since it invovles model training. \n",
    "        \n",
    "        For using word2vec to predict PII data, it is recommended to update the model with new sentences. \n",
    "        For fastttext, it is not necessary since it will infer from the character n-grams. The fasttext training\n",
    "        is much longer than word2vec. \n",
    "        \"\"\"\n",
    "        gensim_X = self._pd_to_gensim_format(text = X)\n",
    "        # update the embedding with new sentences or train the model. \n",
    "        if self.re_train_new_sentences:\n",
    "            self._embedding_training(sentences = gensim_X, update = True)\n",
    "            print(\"transforming while training {} model with new data.\".format(self.algo_name))\n",
    "            \n",
    "            \n",
    "        # extract the PII \n",
    "        extracted_pii_list = [_find_part_pii(text = text, model = self.model)\\\n",
    "                    for text in tqdm(X) ]\n",
    "        \n",
    "        # convert the extracted pii text into vectors.\n",
    "        piivec_matrix = _extracted_pii2matrix(pii_list = extracted_pii_list,\\\n",
    "                                          model = self.model)\n",
    "        return piivec_matrix \n",
    "                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "algo_test_data = pd.read_csv(\"../data/train_text_with_pii_2019_01_05_02_48_24_796403.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 8181.51it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 92377.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00217615,  0.00796351, -0.00748214, ...,  0.00887035,\n",
       "        -0.00148853,  0.00133624],\n",
       "       [-0.00447374,  0.01026269, -0.00777159, ...,  0.0162302 ,\n",
       "         0.00067097,  0.00166161],\n",
       "       [-0.0069133 ,  0.00655595, -0.00614706, ...,  0.00911191,\n",
       "         0.00302767, -0.00566331],\n",
       "       ..., \n",
       "       [-0.02315124,  0.026426  , -0.0140119 , ...,  0.03725655,\n",
       "        -0.00016703, -0.00462752],\n",
       "       [-0.02108077,  0.01676678, -0.00663171, ...,  0.02916127,\n",
       "        -0.00657078,  0.00124387],\n",
       "       [-0.01856113,  0.01454719, -0.01343988, ...,  0.02961764,\n",
       "        -0.00293513, -0.00825683]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding = word_embedding(algo_name = 'word2vec')\n",
    "testing_embedding.fit(algo_test_data['Text']);\n",
    "test_pii_matrix = testing_embedding.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training word2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 2534.26it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 154585.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0752305 , -0.18665323, -0.9611361 , ...,  0.15208749,\n",
       "         0.02958794, -0.41311947],\n",
       "       [ 0.43034682, -0.05534443, -0.96178532, ...,  0.46109006,\n",
       "         0.22167945, -0.54757804],\n",
       "       [ 0.03700855, -0.02521761, -0.01220291, ..., -0.05585773,\n",
       "         0.20333029,  0.08629275],\n",
       "       ..., \n",
       "       [-0.04278483, -0.05869394,  0.02729131, ..., -0.08618189,\n",
       "         0.08368408,  0.00944143],\n",
       "       [ 0.29659262, -0.58743894,  0.01724759, ...,  0.43597522,\n",
       "        -0.06645637,  1.55932295],\n",
       "       [ 0.14534651, -0.12933674, -0.10637662, ...,  0.02798184,\n",
       "         0.06007978,  0.08667902]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained = Word2Vec.load(\"./word2vec/word2vec_cleaned_300_.bin\")\n",
    "testing_pre_trained = word_embedding(algo_name = \"word2vec\", pre_train = pre_trained)\n",
    "testing_pre_trained.fit(algo_test_data[\"Text\"])\n",
    "testing_pre_trained.transform(algo_test_data[\"Text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 6674.95it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 134583.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming while training fasttext model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.00887869,  0.00361352, -0.01853686, ..., -0.05992417,\n",
       "        -0.00419788,  0.05287892],\n",
       "       [ 0.00052317,  0.00261651, -0.01038469, ..., -0.0329863 ,\n",
       "        -0.00088623,  0.02751647],\n",
       "       [-0.0102258 ,  0.00307373, -0.02907217, ..., -0.07936461,\n",
       "        -0.00811043,  0.06561628],\n",
       "       ..., \n",
       "       [-0.02260031,  0.00858459, -0.05097585, ..., -0.14817157,\n",
       "        -0.00801586,  0.1313304 ],\n",
       "       [-0.02994826,  0.0141341 , -0.06360444, ..., -0.16948107,\n",
       "        -0.00543688,  0.1463099 ],\n",
       "       [-0.02625905,  0.0081979 , -0.05881318, ..., -0.15292889,\n",
       "        -0.00570232,  0.13339898]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding = word_embedding(algo_name = 'fasttext')\n",
    "\n",
    "testing_embedding.fit(algo_test_data[\"Text\"]);\n",
    "test_pii_matrix = testing_embedding.transform(algo_test_data[\"Text\"])\n",
    "test_pii_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using index for the tags\n",
      "transforming while training doc2vec model with new data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:00<00:00, 8090.49it/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 111114.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00194516,  0.00455558, -0.00649501, ...,  0.00442428,\n",
       "         0.0003333 ,  0.00088473],\n",
       "       [-0.00020503,  0.00680893, -0.0075832 , ...,  0.01286311,\n",
       "         0.00291317,  0.00047825],\n",
       "       [-0.00049342,  0.00556684,  0.00063821, ...,  0.01182422,\n",
       "        -0.00324276, -0.00413751],\n",
       "       ..., \n",
       "       [-0.01013561,  0.01485797, -0.01136504, ...,  0.02295436,\n",
       "         0.00547766, -0.00727283],\n",
       "       [-0.01072698,  0.008491  , -0.00622351, ...,  0.0203049 ,\n",
       "        -0.00110666, -0.00171354],\n",
       "       [-0.00733437,  0.01433416, -0.00885574, ...,  0.0186146 ,\n",
       "        -0.00331921, -0.00483936]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_embedding_doc2vec = word_embedding(algo_name = 'doc2vec')\n",
    "testing_embedding_doc2vec.fit(X = algo_test_data['Text'])\n",
    "\n",
    "test__doc2vec_pii_matrix = testing_embedding_doc2vec.transform(algo_test_data['Text'])\n",
    "test__doc2vec_pii_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add transfer_learning option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add dum_file option"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
